{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a5f9b5f-7f90-4c7a-b099-dd4fe78403bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import pyperclip\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f3e4ad16-e7c3-4b32-ba25-46e3df59e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data file\n",
    "df = pd.read_parquet('/Users/stefanfeiler/Desktop/SmartBetter/SmartBetter/data/mlb_for_model.parquet')\n",
    "df['number_of_game_today'] = df['number_of_game_today'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d2eb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign two park_ids and one hour_of_start to the most common park_ids and hour_of_start because there are only 2 obsevations for each of these park_ids and splitting into test and train is impossible.\n",
    "park_value = 'LOS03'\n",
    "df.loc[df['park_id'] == 'WIL02', 'park_id'] = park_value\n",
    "df.loc[df['park_id'] == 'DYE01', 'park_id'] = park_value\n",
    "\n",
    "hour_value = 19\n",
    "df.loc[df['hour_of_start'] == 20, 'hour_of_start'] = hour_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b7b7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e0dc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4198a5c4-410a-41a1-9990-0aa94147f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_category(column_name, data):\n",
    "    \n",
    "    column_index = data.columns.tolist().index(column_name)\n",
    "    \n",
    "    arr = data[column_name].values.reshape(-1,1)\n",
    "    \n",
    "    coder = OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "    onehots = coder.fit_transform(arr)\n",
    "    \n",
    "    encoders[column_name] = coder\n",
    "    \n",
    "    return onehots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f47e2b58-e372-4faa-987e-973b5b144717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numeric(column_name, data):\n",
    "    \n",
    "    # Get the index from the full df of the input column name\n",
    "    column_index = data.columns.tolist().index(column_name)\n",
    "    \n",
    "    # Make an array of the column values \n",
    "    arr = data[column_name].values.reshape(-1,1).astype('float')\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "586ff161-3075-44c5-a4a8-492d1e7895ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(input_df):\n",
    "    data = input_df\n",
    "    return_data = np.concatenate(\n",
    "        [\n",
    "            add_numeric('barstool_1_odds', data),\n",
    "            add_numeric('betclic_1_odds', data),\n",
    "            add_numeric('betfair_1_odds', data),\n",
    "            add_numeric('betfred_1_odds', data),\n",
    "            add_numeric('betmgm_1_odds', data),\n",
    "            add_numeric('betonlineag_1_odds', data),\n",
    "            add_numeric('betrivers_1_odds', data),\n",
    "            add_numeric('betus_1_odds', data),\n",
    "            add_numeric('betway_1_odds', data),\n",
    "            add_numeric('bovada_1_odds', data),\n",
    "            add_numeric('casumo_1_odds', data),\n",
    "            add_numeric('circasports_1_odds', data),\n",
    "            add_numeric('coral_1_odds', data),\n",
    "            add_numeric('draftkings_1_odds', data),\n",
    "            add_numeric('fanduel_1_odds', data),\n",
    "            add_numeric('foxbet_1_odds', data),\n",
    "            add_numeric('gtbets_1_odds', data),\n",
    "            add_numeric('ladbrokes_1_odds', data),\n",
    "            add_numeric('lowvig_1_odds', data),\n",
    "            add_numeric('marathonbet_1_odds', data),\n",
    "            add_numeric('matchbook_1_odds', data),\n",
    "            add_numeric('mrgreen_1_odds', data),\n",
    "            add_numeric('mybookieag_1_odds', data),\n",
    "            add_numeric('nordicbet_1_odds', data),\n",
    "            add_numeric('onexbet_1_odds', data),\n",
    "            add_numeric('paddypower_1_odds', data),\n",
    "            add_numeric('pinnacle_1_odds', data),\n",
    "            add_numeric('pointsbetus_1_odds', data),\n",
    "            add_numeric('sport888_1_odds', data),\n",
    "            add_numeric('sugarhouse_1_odds', data),\n",
    "            add_numeric('superbook_1_odds', data),\n",
    "            add_numeric('twinspires_1_odds', data),\n",
    "            add_numeric('unibet_1_odds', data),\n",
    "            add_numeric('unibet_eu_1_odds', data),\n",
    "            add_numeric('unibet_uk_1_odds', data),\n",
    "            add_numeric('unibet_us_1_odds', data),\n",
    "            add_numeric('williamhill_1_odds', data),\n",
    "            add_numeric('williamhill_us_1_odds', data),\n",
    "            add_numeric('wynnbet_1_odds', data),\n",
    "            add_numeric('minutes_since_commence', data),\n",
    "            add_numeric('this_team_game_of_season', data),\n",
    "            add_numeric('opponent_game_of_season', data),\n",
    "            \n",
    "            add_category('home_away', data),\n",
    "            add_category('team_1', data),\n",
    "            add_category('hour_of_start', data),\n",
    "            add_category('day_of_week', data),\n",
    "            add_category('number_of_game_today', data),\n",
    "            add_category('day_night', data),\n",
    "            add_category('park_id', data),\n",
    "            add_category('this_team_league', data),\n",
    "            add_category('opponent_league', data),\n",
    "        ],\n",
    "        1\n",
    "    )\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a9da77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_game_ids = df['game_id'].unique()\n",
    "# unique_game_ids_train, unique_game_ids_test = train_test_split(unique_game_ids, test_size=0.2, random_state=42)\n",
    "# training_data = df[df['game_id'].isin(unique_game_ids_train)]\n",
    "# testing_data = df[df['game_id'].isin(unique_game_ids_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf5710d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain unique game IDs\n",
    "unique_game_ids = df['game_id'].unique()\n",
    "\n",
    "# Split unique game IDs into train, test, and validation sets\n",
    "unique_game_ids_train, unique_game_ids_holdout = train_test_split(unique_game_ids, test_size=0.3, random_state=42)\n",
    "unique_game_ids_test, unique_game_ids_val = train_test_split(unique_game_ids_holdout, test_size=0.5, random_state=42)\n",
    "\n",
    "# Filter the original DataFrame based on the train, test, and validation game IDs\n",
    "training_data = df[df['game_id'].isin(unique_game_ids_train)]\n",
    "testing_data = df[df['game_id'].isin(unique_game_ids_test)]\n",
    "validation_data = df[df['game_id'].isin(unique_game_ids_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af330101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hour_of_start  game_id\n",
      "0               8       10\n",
      "6              14       47\n",
      "1               9       95\n",
      "4              12      245\n",
      "11             19      273\n",
      "3              11      441\n",
      "7              15      453\n",
      "5              13      573\n",
      "2              10      579\n",
      "10             18      616\n",
      "9              17      840\n",
      "8              16     1632\n"
     ]
    }
   ],
   "source": [
    "# Group by 'park_id' and count the number of unique 'game_id' values\n",
    "park_id_game_count = df.groupby('hour_of_start')['game_id'].nunique().reset_index()\n",
    "\n",
    "# Sort the counts in ascending order\n",
    "sorted_counts = park_id_game_count.sort_values('game_id')\n",
    "\n",
    "# Display the sorted counts\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d20de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_y = df['target']\n",
    "full_data = df.drop('target', axis='columns')\n",
    "\n",
    "training_y = training_data['target']\n",
    "training_data = training_data.drop('target', axis='columns')\n",
    "\n",
    "testing_y = testing_data['target']\n",
    "testing_data = testing_data.drop('target', axis='columns')\n",
    "\n",
    "validation_y = validation_data['target']\n",
    "validation_data = validation_data.drop('target', axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a33b261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = make_data(df)\n",
    "training_data = make_data(training_data)\n",
    "testing_data = make_data(testing_data)\n",
    "validation_data = make_data(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5bdc7303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(705958, 134)\n",
      "(492668, 134)\n",
      "(104948, 134)\n",
      "(108342, 133)\n"
     ]
    }
   ],
   "source": [
    "print(full_data.shape)\n",
    "print(training_data.shape)\n",
    "print(testing_data.shape)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c391959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.05, 2.2 , 2.14, ..., 0.  , 0.  , 0.  ],\n",
       "       [2.17, 2.15, 1.53, ..., 0.  , 0.  , 0.  ],\n",
       "       [2.02, 2.15, 1.96, ..., 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [0.  , 0.  , 2.28, ..., 2.2 , 2.25, 0.  ],\n",
       "       [0.  , 0.  , 2.28, ..., 2.2 , 2.25, 0.  ],\n",
       "       [0.  , 0.  , 2.28, ..., 2.2 , 2.25, 0.  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data[:, :39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e006675c-e96b-479f-8ed2-2eaacb72a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indices of the columns you want to standardize and those we don't\n",
    "continuous_vars = full_data[:, :42]\n",
    "categorical_vars = full_data[:, 42:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a7e364d-ba18-49f3-89a0-7032f30fe5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of StandardScaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(continuous_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "734ab3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_vars_train = training_data[:, :42]\n",
    "categorical_vars_train = training_data[:, 42:]\n",
    "\n",
    "continuous_vars_test = testing_data[:, :42]\n",
    "categorical_vars_test = testing_data[:, 42:]\n",
    "\n",
    "continuous_vars_full = full_data[:, :42]\n",
    "categorical_vars_full = full_data[:, 42:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "475f7eac-083b-4332-b2e0-537753254d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the columns of the dataset\n",
    "X_train = np.hstack((scaler.transform(continuous_vars_train), categorical_vars_train))\n",
    "X_test = np.hstack((scaler.transform(continuous_vars_test), categorical_vars_test))\n",
    "X_full = np.hstack((scaler.transform(continuous_vars_full), categorical_vars_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5735168a-0cc9-4ec2-9e3e-0eb8a49e0973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260770, 134)\n",
      "(64975, 134)\n",
      "(325745, 134)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6ab2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert input data to numpy arrays\n",
    "X_train_np = X_train.astype(np.float32)\n",
    "X_test_np = X_test.astype(np.float32)\n",
    "y_train_np = training_y.values.astype(np.float32)  # Convert y_train to numpy array\n",
    "y_test_np = testing_y.values.astype(np.float32)    # Convert y_test to numpy array\n",
    "\n",
    "# Create Torch datasets with the numpy arrays\n",
    "train_data = torch.utils.data.TensorDataset(torch.tensor(X_train_np), torch.tensor(y_train_np))\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_test_np), torch.tensor(y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d8fec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbe132b55f0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1894c9bd-ade7-47ef-9a65-45cce1836c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loaders for each of our datasets \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6495d47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260770, 134)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "510a2e49-65ea-4b6a-a1a9-0cdb8a34cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the layers and activation functions of our model\n",
    "# model = torch.nn.Sequential(   \n",
    "#     torch.nn.Linear(134,256),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(256,256),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(256,128),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(128,64),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(64,16),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(16,1)\n",
    "# )\n",
    "# params_count = (134*256)+(256*256)+(256*128)+(128*64)+(64*16)+16\n",
    "# params_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efed2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the layers and activation functions of our model\n",
    "# model = torch.nn.Sequential(   \n",
    "#     torch.nn.Linear(134,256),\n",
    "#     torch.nn.Sigmoid(),\n",
    "#     torch.nn.Linear(256,256),\n",
    "#     torch.nn.Sigmoid(),\n",
    "#     torch.nn.Linear(256,128),\n",
    "#     torch.nn.Sigmoid(),\n",
    "#     torch.nn.Linear(128,64),\n",
    "#     torch.nn.Sigmoid(),\n",
    "#     torch.nn.Linear(64,16),\n",
    "#     torch.nn.Sigmoid(),\n",
    "#     torch.nn.Linear(16,1)\n",
    "# )\n",
    "# params_count = (134*256)+(256*256)+(256*128)+(128*64)+(64*16)+16\n",
    "# params_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "151bc190-5357-4fa6-ba2a-842a0d69b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the layers and activation functions of our model\n",
    "# model = torch.nn.Sequential(   \n",
    "#     torch.nn.Linear(137,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,256),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(256,1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3943e9c8-af08-45db-bc6d-ad7b28e5aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines our scoring function\n",
    "def scoring_function(pred, label):\n",
    "    return nn.functional.binary_cross_entropy_with_logits(pred, label)\n",
    "\n",
    "# Defines number of epochs we want to train through\n",
    "# num_epochs = 1\n",
    "\n",
    "# Defines our optimizer and the learning rate \n",
    "# optimizer = torch.optim.Adam( model.parameters(), lr=.001)#  , weight_decay=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4425cb01-94bd-41dd-a102-f3ea5ef78e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_stats(c_tensor, i_tensor):\n",
    "    unscaled_correct = scaler.inverse_transform(c_tensor[:, :42]) if len(c_tensor) > 0 else np.array([]) # unscale the continuous variables\n",
    "    unscaled_incorrect = scaler.inverse_transform(i_tensor[:, :42]) if len(i_tensor) > 0 else np.array([]) # unscale the continuous variables\n",
    "\n",
    "    amt_correct_predictions = len(unscaled_correct) # amount of correct predictions\n",
    "    amt_incorrect_predictions = len(unscaled_incorrect) # amount of incorrect predictions\n",
    "    amt_total_bets = (amt_correct_predictions + amt_incorrect_predictions) # amount of total bets placed\n",
    "\n",
    "    if len(unscaled_correct) > 0:\n",
    "        unscaled_correct = unscaled_correct[:, :39] # only odds data\n",
    "        sum = 0 # calculate the average market odds\n",
    "        count = 0\n",
    "        for row in unscaled_correct:\n",
    "            for val in row:\n",
    "                if val > 0.01: # don't average the 'missing' (0) values \n",
    "                    sum += val\n",
    "                    count +=1\n",
    "        average_odds_won = sum / count\n",
    "    else:\n",
    "        average_odds_won = 0\n",
    "\n",
    "    money_profited = ((amt_correct_predictions * average_odds_won * 100) - (100 * amt_correct_predictions))\n",
    "\n",
    "    money_lost = (amt_incorrect_predictions * 100)\n",
    "\n",
    "    p_l = money_profited - money_lost\n",
    "    \n",
    "    if amt_total_bets > 0: \n",
    "        ev_per_bet = p_l/amt_total_bets\n",
    "        correct_pred_percet = amt_correct_predictions/amt_total_bets\n",
    "    else:\n",
    "        correct_pred_percet = 0\n",
    "        ev_per_bet = 0\n",
    "\n",
    "    return p_l, correct_pred_percet, ev_per_bet, amt_total_bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0432d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_stats(c_list, i_list):\n",
    "    odds_sum = 0 \n",
    "    odds_count = 0\n",
    "    correct_preds_count = 0\n",
    "    incorrect_preds_count = 0\n",
    "\n",
    "    # For each batch:\n",
    "    for c_tensor in c_list:\n",
    "        unscaled_correct = scaler.inverse_transform(c_tensor[:, :42]) if len(c_tensor) > 0 else np.array([]) # unscale the continuous variables\n",
    "        amt_correct_predictions = len(unscaled_correct) # amount of correct predictions\n",
    "        correct_preds_count += amt_correct_predictions\n",
    "\n",
    "        if len(unscaled_correct) > 0:\n",
    "            unscaled_correct = unscaled_correct[:, :39] # only odds data\n",
    "            for row in unscaled_correct:\n",
    "                for val in row:\n",
    "                    if val > 0.01: # don't average the 'missing' (0) values \n",
    "                        odds_sum += val\n",
    "                        odds_count +=1\n",
    "    \n",
    "    for i_tensor in i_list:\n",
    "        unscaled_incorrect = scaler.inverse_transform(i_tensor[:, :42]) if len(i_tensor) > 0 else np.array([]) # unscale the continuous variables\n",
    "        amt_incorrect_predictions = len(unscaled_incorrect) # amount of incorrect predictions\n",
    "        incorrect_preds_count += amt_incorrect_predictions\n",
    "        \n",
    "\n",
    "    \n",
    "    average_odds_won = odds_sum / odds_count\n",
    "    amt_total_bets = (correct_preds_count + incorrect_preds_count) # amount of total bets placed\n",
    "    money_profited = ((correct_preds_count * average_odds_won * 100) - (100 * correct_preds_count))\n",
    "    money_lost = (incorrect_preds_count * 100)\n",
    "\n",
    "    total_p_l = money_profited - money_lost\n",
    "\n",
    "    if amt_total_bets > 0: \n",
    "        ev_per_bet = total_p_l/amt_total_bets\n",
    "        correct_pred_percet = correct_preds_count/amt_total_bets\n",
    "    else:\n",
    "        correct_pred_percet = 0\n",
    "        ev_per_bet = 0\n",
    "\n",
    "    return total_p_l, correct_pred_percet, ev_per_bet, amt_total_bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "356d1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(tpr, fpr):\n",
    "    sorted_indices = np.argsort(fpr)  # Sort based on FPR\n",
    "    sorted_tpr = np.array(tpr)[sorted_indices]\n",
    "    sorted_fpr = np.array(fpr)[sorted_indices]\n",
    "\n",
    "    # Calculate the AUC using the sorted TPR and FPR arrays\n",
    "    auc = np.trapz(sorted_tpr, sorted_fpr)\n",
    "\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dbe12f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresholds(my_thresholds, input_evs_per_bet, input_amounts_of_bets, input_precisions, input_auc):\n",
    "\n",
    "  my_list = []\n",
    "\n",
    "  thresholds_new = np.array([t.detach().numpy() for t in my_thresholds])\n",
    "  evs_per_bet = np.array(input_evs_per_bet)\n",
    "  amounts_of_bets = np.array(input_amounts_of_bets)\n",
    "  precisions = np.array(input_precisions)\n",
    "\n",
    "\n",
    "  min_bets = 1373\n",
    "  max_bets = 6864\n",
    "\n",
    "  # Select the subset that fits our bet frequency criteria\n",
    "  filtered_amounts_of_bets = amounts_of_bets[(amounts_of_bets > min_bets) & (amounts_of_bets < max_bets)]\n",
    "  filtered_evs = evs_per_bet[(amounts_of_bets > min_bets) & (amounts_of_bets < max_bets)]\n",
    "  filtered_precisions = precisions[(amounts_of_bets > min_bets) & (amounts_of_bets < max_bets)]\n",
    "  filtered_thresholds = thresholds_new[(amounts_of_bets > min_bets) & (amounts_of_bets < max_bets)]\n",
    "  # Now sort by precisions\n",
    "  sorted_indices = np.argsort(filtered_evs)[::-1]\n",
    "  # sorted_indices = np.argsort(filtered_precisions)[::-1]\n",
    "  \n",
    "  sorted_filtered_precisions = filtered_precisions[sorted_indices]\n",
    "  sorted_filtered_evs = filtered_evs[sorted_indices]\n",
    "  sorted_filtered_thresholds = filtered_thresholds[sorted_indices]\n",
    "  sorted_filtered_amounts = filtered_amounts_of_bets[sorted_indices]\n",
    "\n",
    "  # Now select the best 5\n",
    "  sorted_filtered_precisions_best = sorted_filtered_precisions[:5]\n",
    "  sorted_filtered_evs_best = sorted_filtered_evs[:5]\n",
    "  sorted_filtered_thresholds_best = sorted_filtered_thresholds[:5]\n",
    "  sorted_filtered_amounts_best = sorted_filtered_amounts[:5]\n",
    "\n",
    "  my_list.append(input_auc)\n",
    "  for i in range(len(sorted_filtered_precisions_best)):\n",
    "    my_list.append(sorted_filtered_thresholds_best[i])\n",
    "    my_list.append(sorted_filtered_amounts_best[i])\n",
    "    my_list.append(sorted_filtered_evs_best[i])\n",
    "    my_list.append(sorted_filtered_precisions_best[i])\n",
    "\n",
    "  return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd82fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats_full_train(list_predictions, list_targets, list_batchx, sigmoid):\n",
    "    # We have fully trained our model\n",
    "\n",
    "    # Define the thresholds we're testing based on the model architecture\n",
    "    thresholds = []\n",
    "\n",
    "    # Define some lists whose values we want to see change across thresholds\n",
    "    tprs = []\n",
    "\n",
    "    fprs = []\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    amounts_of_bets = []\n",
    "\n",
    "    evs_per_bet = []\n",
    "\n",
    "    # Gets data regarding the whole run \n",
    "    predictions = torch.cat(list_predictions, dim=0)\n",
    "    targets = torch.cat(list_targets, dim=0)\n",
    "    x_vals = torch.cat(list_batchx, dim=0)\n",
    "\n",
    "    if sigmoid:\n",
    "        for value in range(1, 101):\n",
    "          threshold = value / 100.0\n",
    "          thresholds.append(threshold)\n",
    "    else:\n",
    "       # Find the minimum and maximum values\n",
    "        min_value = torch.min(predictions)\n",
    "        max_value = torch.max(predictions)\n",
    "        # Calculate the range and step size\n",
    "        value_range = max_value - min_value\n",
    "        step_size = value_range / 100.0\n",
    "        # Step through the range\n",
    "        for i in range(101):\n",
    "            threshold = min_value + i * step_size\n",
    "            thresholds.append(threshold)\n",
    "    \n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Defines some variables \n",
    "        odds_sum = 0 \n",
    "        bet_count = 0\n",
    "        row_sum = 0\n",
    "        row_count = 0\n",
    "\n",
    "        thresh_predictions = torch.where(predictions > threshold, 1, 0)\n",
    "\n",
    "        thresh_predictions = thresh_predictions.squeeze()\n",
    "        \n",
    "        # Splits our sets \n",
    "        true_pos = x_vals[(thresh_predictions == 1) & (targets == 1)] # True positives\n",
    "\n",
    "        false_pos = x_vals[(thresh_predictions == 1) & (targets == 0)] # False positives\n",
    "        \n",
    "        true_neg = x_vals[(thresh_predictions == 0) & (targets == 0)] # True negatives\n",
    "        false_neg = x_vals[(thresh_predictions == 0) & (targets == 1)] # False negatives\n",
    "\n",
    "        # Gets info about our sets\n",
    "        amount_of_correct_pos_preds = true_pos.shape[0]\n",
    "        amount_of_incorrect_pos_preds = false_pos.shape[0]\n",
    "        amount_of_bets = amount_of_correct_pos_preds + amount_of_incorrect_pos_preds\n",
    "        \n",
    "        # Unscale our true_pos set\n",
    "        unscaled_true_pos = scaler.inverse_transform(true_pos[:, :42]) if len(true_pos) > 0 and hasattr(scaler, 'scale_') else np.array([])\n",
    "\n",
    "        # If we have any amount of true positive predictions\n",
    "        if len(unscaled_true_pos) > 0:\n",
    "                # Select only the odds data\n",
    "                unscaled_true_pos = unscaled_true_pos[:, :39] # only odds data\n",
    "                # Iterate through each prediciton\n",
    "                for row in unscaled_true_pos:\n",
    "\n",
    "                    # Get the market average for that bet\n",
    "                    row_sum = 0\n",
    "                    row_count = 0\n",
    "                    for val in row:\n",
    "                        if val > 0.01 and val < 100: # don't average the 'missing' (0) values \n",
    "                            row_sum += val\n",
    "                            row_count +=1\n",
    "                    # If we have non-zero values in that row\n",
    "                    if row_count > 0:\n",
    "                        bet_avg_odds = row_sum / row_count\n",
    "                        # Add this bets odds to the average \n",
    "                        odds_sum += bet_avg_odds\n",
    "                        bet_count +=1\n",
    "                    \n",
    "        # Average odds across all positive predictions for this whole run\n",
    "        if bet_count > 0:\n",
    "            avg_odds = odds_sum / bet_count\n",
    "            \n",
    "            \n",
    "            gross_p_l = (avg_odds * 100 * bet_count) - (bet_count * 100)\n",
    "\n",
    "            net_p_l = gross_p_l - (100 * amount_of_incorrect_pos_preds)\n",
    "\n",
    "            ev_per_bet = net_p_l / bet_count\n",
    "            tpr = true_pos.shape[0] / (true_pos.shape[0] + false_neg.shape[0])\n",
    "            fpr = false_pos.shape[0] / (false_pos.shape[0] + true_neg.shape[0])\n",
    "            precision = true_pos.shape[0] / (true_pos.shape[0] + false_pos.shape[0])\n",
    "\n",
    "\n",
    "        elif bet_count == 0:\n",
    "            avg_odds = 0\n",
    "            net_p_l = 0\n",
    "            ev_per_bet = 0\n",
    "            tpr = 0\n",
    "            fpr = 0\n",
    "            precision = 0\n",
    "            \n",
    "\n",
    "        tprs.append(tpr)\n",
    "\n",
    "        fprs.append(fpr)\n",
    "\n",
    "        precisions.append(precision)\n",
    "\n",
    "        amounts_of_bets.append(amount_of_bets)\n",
    "\n",
    "        evs_per_bet.append(ev_per_bet)\n",
    "            \n",
    "    auc = calculate_auc(tprs, fprs)\n",
    "\n",
    "    info_list = find_best_thresholds(thresholds, evs_per_bet, amounts_of_bets, precisions, auc)\n",
    "    \n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1df6487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tpr_fpr_auc(list_predictions, list_targets, list_batchx):\n",
    "    predictions = torch.cat(list_predictions, dim=0)\n",
    "\n",
    "    targets = torch.cat(list_targets, dim=0)\n",
    "\n",
    "    x_vals = torch.cat(list_batchx, dim=0)\n",
    "\n",
    "    tprs = []\n",
    "\n",
    "    fprs = []\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    min_pred = torch.min(predictions)\n",
    "\n",
    "    max_pred = torch.min(predictions)\n",
    "\n",
    "    # Convert the range step size to integers\n",
    "    step_size = 0.01\n",
    "\n",
    "    num_steps = int((max_pred - min_pred) / step_size)\n",
    "\n",
    "\n",
    "    for value in range(1, 101):\n",
    "\n",
    "        threshold = value / 100.0\n",
    "\n",
    "        thresh_predictions = torch.where(predictions > threshold, 1, 0)\n",
    "\n",
    "        true_pos = x_vals[(thresh_predictions.squeeze() == 1) & (targets.squeeze() == 1)] # True positives\n",
    "        false_pos = x_vals[(thresh_predictions.squeeze() == 1) & (targets.squeeze() == 0)] # False positives\n",
    "        true_neg = x_vals[(thresh_predictions.squeeze() == 0) & (targets.squeeze() == 0)] # True negatives\n",
    "        false_neg = x_vals[(thresh_predictions.squeeze() == 0) & (targets.squeeze() == 1)] # False negatives\n",
    "\n",
    "        tpr = true_pos.shape[0] / (true_pos.shape[0] + false_neg.shape[0])\n",
    "\n",
    "        fpr = false_pos.shape[0] / (false_pos.shape[0] + true_neg.shape[0])\n",
    "\n",
    "        precision = true_pos.shape[0] / (true_pos.shape[0] + false_pos.shape[0])\n",
    "\n",
    "        tprs.append(tpr)\n",
    "\n",
    "        fprs.append(fpr)\n",
    "\n",
    "        precisions.append(precision)\n",
    "\n",
    "    auc = calculate_auc(tprs, fprs)\n",
    "\n",
    "    print(f'Precisions: {precisions}')\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe431f8d-9002-4e9d-8962-4d8fe2b424f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, positive_weight):\n",
    "    # Initializes a list that will contain our batch losses for an individual epoch\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Defines how we want to step through each batch in the epoch\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # Resets the gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare the input and output tensors for the current batch\n",
    "        batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
    "        batchY = torch.tensor(batch[1], dtype=torch.float32)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model.forward(batchX)\n",
    "        batchY = batchY.unsqueeze(1)  # Reshape to (batch_size, 1)\n",
    "        \n",
    "        # Compute the loss with weighted BCEWithLogitsLoss\n",
    "        pos_weight = torch.tensor([positive_weight])  # higher weight for positive class\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        #criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        loss = criterion(y_pred, batchY)\n",
    "        \n",
    "        # Store the loss for this batch in the list\n",
    "        epoch_losses.append(loss.detach().clone())\n",
    "\n",
    "        # Compute the gradient of the error with respect to the model parameters\n",
    "        loss.mean().backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    all_epoch_loss = torch.tensor(epoch_losses)\n",
    "    epoch_loss = torch.mean(all_epoch_loss)\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04109a9f-401f-485c-be10-b990a3323b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    epoch_correct_pred_tensors = []\n",
    "    epoch_incorrect_pred_tensors = []\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Loop over the test data\n",
    "    for batch in test_loader:\n",
    "        # Prepare the input and output tensors for the current batch\n",
    "        batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
    "        batchY = torch.tensor(batch[1], dtype=torch.float32)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model(batchX)\n",
    "\n",
    "        # Apply threshold\n",
    "        # # TODO: HYPERPARAM\n",
    "        # preds_numpy = predictions.detach().numpy()\n",
    "\n",
    "        # preds_list.append(preds_numpy)\n",
    "\n",
    "        predictions = torch.where((predictions > 5) & (predictions < 15), 1, 0)\n",
    "\n",
    "        correct_pred_rows = batchX[(predictions.squeeze() == 1) & (batchY.squeeze() == 1)] # find all rows where we make a correct win prediction\n",
    "\n",
    "        incorrect_pred_rows = batchX[(predictions.squeeze() == 1) & (batchY.squeeze() == 0)] # find all rows where we make an incorrect win prediction\n",
    "\n",
    "        epoch_correct_pred_tensors.append(correct_pred_rows)\n",
    "\n",
    "        epoch_incorrect_pred_tensors.append(incorrect_pred_rows)\n",
    "\n",
    "    # Return the results\n",
    "    return calc_epoch_stats(epoch_correct_pred_tensors, epoch_incorrect_pred_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad36b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_tpr_fpr_auc():\n",
    "    # New list for all of the batch predicitons \n",
    "    all_predictions = []\n",
    "\n",
    "    # New list for all of the batch targets \n",
    "    all_targets = []\n",
    "\n",
    "    # New list for each batch\n",
    "    all_batchx = []\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Loop over the test data\n",
    "    for batch in test_loader:\n",
    "        # Prepare the input and output tensors for the current batch\n",
    "        batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
    "        batchY = torch.tensor(batch[1], dtype=torch.float32)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model(batchX)\n",
    "\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "        all_targets.append(batchY)\n",
    "\n",
    "        all_batchx.append(batchX)\n",
    "\n",
    "    auc = calc_tpr_fpr_auc(all_predictions, all_targets, all_batchx)\n",
    "\n",
    "    # Return the results\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d291a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_test(model, sigmoid):\n",
    "    # New list for all of the batch predicitons \n",
    "    all_predictions = []\n",
    "\n",
    "    # New list for all of the batch targets \n",
    "    all_targets = []\n",
    "\n",
    "    # New list for each batch\n",
    "    all_batchx = []\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Loop over the test data\n",
    "    for batch in test_loader:\n",
    "        # Prepare the input and output tensors for the current batch\n",
    "        batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
    "        batchY = torch.tensor(batch[1], dtype=torch.float32)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model(batchX)\n",
    "\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "        all_targets.append(batchY)\n",
    "\n",
    "        all_batchx.append(batchX)\n",
    "\n",
    "    #auc = calc_tpr_fpr_auc(all_predictions, all_targets, all_batchx)\n",
    "    stats = calc_stats_full_train(all_predictions, all_targets, all_batchx, sigmoid)\n",
    "\n",
    "    # Return the results\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "40f6f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(architecture, my_lr, my_weight_decay, my_num_epochs, my_batch_size):\n",
    "    print(f'Architecture: {architecture} learning rate: {my_lr} weight decay: {my_weight_decay} num epochs: {my_num_epochs} batch size: {my_batch_size}')\n",
    "\n",
    "    if architecture == 'sigmoid':\n",
    "        model = torch.nn.Sequential(   \n",
    "            torch.nn.Linear(134,256),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(256,256),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(256,128),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(128,64),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(64,16),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(16,1)\n",
    "        )\n",
    "    elif architecture == 'relu': \n",
    "        model = torch.nn.Sequential(   \n",
    "            torch.nn.Linear(134,256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256,256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256,128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,1)\n",
    "        )\n",
    "    elif architecture == 'silu':\n",
    "        model = torch.nn.Sequential(   \n",
    "            torch.nn.Linear(134,256),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(256,256),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(256,128),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(128,64),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(64,16),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(16,1)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eedd5d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: silu learning rate: 0.001 weight decay: on num epochs: 5 batch size: 4096\n",
      "AUC: 0.6229795988559543\n",
      "Model 1: 15 trained successfully. Best EV: 10.203719589059382\n"
     ]
    }
   ],
   "source": [
    "hyper_params = pd.read_csv('/Users/stefanfeiler/Desktop/SmartBetter/SmartBetter/data/tuning_info_test.csv')\n",
    "stats_list = []\n",
    "column_names = ['auc', 'thresh_1', 'tbp_1', 'ev_1', 'prec_1', 'thresh_2', 'tbp_2', 'ev_2', 'prec_2', 'thresh_3', 'tbp_3', 'ev_3', 'prec_3', 'thresh_4', 'tbp_4', 'ev_4', 'prec_4', 'thresh_5', 'tbp_5', 'ev_5', 'prec_5']\n",
    "cols = pd.DataFrame(columns=column_names)\n",
    "cols.to_csv('/Users/stefanfeiler/Desktop/SmartBetter/SmartBetter/data/tuning_info.csv', index=False)\n",
    "\n",
    "# for i in range(len(hyper_params)):\n",
    "for i in range(1):\n",
    "  architecture = hyper_params['architecture'].iloc[i]\n",
    "  lr = hyper_params['learning_rate'].iloc[i]\n",
    "  weight_decay = hyper_params['weight_decay'].iloc[i]\n",
    "  num_epochs = int(hyper_params['num_epochs'].iloc[i])\n",
    "  batch_size = int(hyper_params['batch_size'].iloc[i])\n",
    "  pos_weight = int(hyper_params['pos_weight'].iloc[i])\n",
    "  if architecture == 'sigmoid':\n",
    "    sigmoid = True\n",
    "  elif architecture != 'sigmoid':\n",
    "    sigmoid = False\n",
    "\n",
    "  model = define_model(architecture, lr, weight_decay, num_epochs, batch_size)\n",
    "\n",
    "  optimizer = torch.optim.Adam( model.parameters(), lr=lr)#  , weight_decay=.0001)\n",
    "\n",
    "\n",
    "  # Set up loaders for each of our datasets \n",
    "  train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "  test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "  if weight_decay == 'on':\n",
    "      optimizer = torch.optim.Adam( model.parameters(), lr=lr, weight_decay=.0001)\n",
    "  else:\n",
    "      optimizer = torch.optim.Adam( model.parameters(), lr=lr)\n",
    "\n",
    "  best_loss = float('inf')\n",
    "\n",
    "  patience = 3  # Number of epochs without improvement before stopping\n",
    "\n",
    "  epochs_without_improvement = 0\n",
    "\n",
    "  # Calls the train function for each of our epochs, prints the running results\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    ep_result = train(model, pos_weight)\n",
    "\n",
    "    # Check if the validation loss has improved\n",
    "    if ep_result < best_loss:\n",
    "        best_loss = ep_result\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # Check if the training should stop\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping: No improvement in training loss.\")\n",
    "        break\n",
    "  \n",
    "  # Gets the testing data\n",
    "  info_list = tune_model_test(model, sigmoid)\n",
    "\n",
    "  # do something with stats \n",
    "  info_df = pd.DataFrame(info_list).T\n",
    "\n",
    "  info_df.to_csv('/Users/stefanfeiler/Desktop/SmartBetter/SmartBetter/data/tuning_info.csv', mode='a', header=False, index=False)\n",
    "\n",
    "  info_df = pd.read_csv('/Users/stefanfeiler/Desktop/SmartBetter/SmartBetter/data/tuning_info.csv')\n",
    "\n",
    "  new_df = pd.concat([hyper_params, info_df], axis = 1)\n",
    "\n",
    "  new_df.to_csv('/Users/stefanfeiler/Desktop/SmartBetter/SmartBetter/data/tuning_results.csv', index=False)\n",
    "\n",
    "  print(f'Model {i + 1}: {len(hyper_params)} trained successfully. Best EV: {info_df[\"ev_1\"].iloc[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is named 'model'\n",
    "torch.save(model.state_dict(), 'MODELS/mlb_model_pikes.pth')\n",
    "\n",
    "# Assuming 'encoder' is your trained OneHotEncoder object\n",
    "with open('MODELS/mlb_encoders_pikes.pkl', 'wb') as file:\n",
    "    pickle.dump(encoders, file)\n",
    "\n",
    "# Save the scaler using pickle\n",
    "with open('MODELS/mlb_scaler_pikes.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = ['test']\n",
    "test2 = ['test2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a570fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list.append([test1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list.append(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17629d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f43498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac954fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd039e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72a515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3e75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896ecdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cec84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780ef9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f92485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554dbcee-2ae6-4855-af9f-a65897dcc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = test_data[:, 20:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c68291-b93c-4164-a875-2ed7b4bd9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecfde7-dac8-4c9e-9908-bbb6e235f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts for each category\n",
    "counts = np.sum(teams, axis=0)\n",
    "\n",
    "print(counts)\n",
    "\n",
    "# Plot the distribution using a bar chart\n",
    "labels = [i for i in range(30)]\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Distribution of Categorical Data')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution using a histogram\n",
    "categories = np.argmax(one_hot_data, axis=1)\n",
    "plt.hist(categories, bins=3, align='left', rwidth=0.5)\n",
    "plt.xticks([0, 1, 2], labels)\n",
    "plt.title('Distribution of Categorical Data')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b9ecb-5375-4abe-b84d-003a3d3411d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)  # convert y_test to a numpy array\n",
    "counts_1 = np.sum(teams[y_test.astype(int) == 1], axis=0)\n",
    "counts_0 = np.sum(teams[y_test.astype(int) == 0], axis=0)\n",
    "counts = np.sum(teams, axis=0)\n",
    "\n",
    "print(counts_1)\n",
    "print(counts_0)\n",
    "print(counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e2967-6334-450d-98c3-041d6aaf48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bars in stack manner\n",
    "x = [i for i in range(30)]\n",
    "plt.bar(x, counts_1, color='r')\n",
    "#plt.bar(x, counts_0, bottom=counts_1, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a539f-b320-4590-9f02-1715bd2655cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
