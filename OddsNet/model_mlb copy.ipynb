{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "3a5f9b5f-7f90-4c7a-b099-dd4fe78403bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "f3e4ad16-e7c3-4b32-ba25-46e3df59e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data file\n",
    "df = pd.read_parquet('/Users/stefanfeiler/Desktop/SmartBetter/SmartBetter/data/mlb_for_model.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "4198a5c4-410a-41a1-9990-0aa94147f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_category(column_name, data):\n",
    "    \n",
    "    column_index = data.columns.tolist().index(column_name)\n",
    "    \n",
    "    arr = data[column_name].values.reshape(-1,1)\n",
    "    \n",
    "    coder = OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "    onehots = coder.fit_transform(arr)\n",
    "    \n",
    "    #print(onehots.shape)\n",
    "    \n",
    "    return onehots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "f47e2b58-e372-4faa-987e-973b5b144717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numeric(column_name, data):\n",
    "    \n",
    "    # Get the index from the full df of the input column name\n",
    "    column_index = data.columns.tolist().index(column_name)\n",
    "    \n",
    "    # Make an array of the column values \n",
    "    arr = data[column_name].values.reshape(-1,1).astype('float')\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "586ff161-3075-44c5-a4a8-492d1e7895ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_data(input_df):\n",
    "    data = input_df\n",
    "    full_data = np.concatenate(\n",
    "        [\n",
    "            add_numeric('barstool_1_odds', data),\n",
    "            add_numeric('betclic_1_odds', data),\n",
    "            add_numeric('betfair_1_odds', data),\n",
    "            add_numeric('betfred_1_odds', data),\n",
    "            add_numeric('betmgm_1_odds', data),\n",
    "            add_numeric('betonlineag_1_odds', data),\n",
    "            add_numeric('betrivers_1_odds', data),\n",
    "            add_numeric('betus_1_odds', data),\n",
    "            add_numeric('betway_1_odds', data),\n",
    "            add_numeric('bovada_1_odds', data),\n",
    "            add_numeric('casumo_1_odds', data),\n",
    "            add_numeric('circasports_1_odds', data),\n",
    "            add_numeric('coral_1_odds', data),\n",
    "            add_numeric('draftkings_1_odds', data),\n",
    "            add_numeric('fanduel_1_odds', data),\n",
    "            add_numeric('foxbet_1_odds', data),\n",
    "            add_numeric('gtbets_1_odds', data),\n",
    "            add_numeric('ladbrokes_1_odds', data),\n",
    "            add_numeric('lowvig_1_odds', data),\n",
    "            add_numeric('marathonbet_1_odds', data),\n",
    "            add_numeric('matchbook_1_odds', data),\n",
    "            add_numeric('mrgreen_1_odds', data),\n",
    "            add_numeric('mybookieag_1_odds', data),\n",
    "            add_numeric('nordicbet_1_odds', data),\n",
    "            add_numeric('onexbet_1_odds', data),\n",
    "            add_numeric('paddypower_1_odds', data),\n",
    "            add_numeric('pinnacle_1_odds', data),\n",
    "            add_numeric('pointsbetus_1_odds', data),\n",
    "            add_numeric('sport888_1_odds', data),\n",
    "            add_numeric('sugarhouse_1_odds', data),\n",
    "            add_numeric('superbook_1_odds', data),\n",
    "            add_numeric('twinspires_1_odds', data),\n",
    "            add_numeric('unibet_1_odds', data),\n",
    "            add_numeric('unibet_eu_1_odds', data),\n",
    "            add_numeric('unibet_uk_1_odds', data),\n",
    "            add_numeric('unibet_us_1_odds', data),\n",
    "            add_numeric('williamhill_1_odds', data),\n",
    "            add_numeric('williamhill_us_1_odds', data),\n",
    "            add_numeric('wynnbet_1_odds', data),\n",
    "            add_numeric('minutes_since_commence', data),\n",
    "            add_numeric('this_team_game_of_season', data),\n",
    "            add_numeric('opponent_game_of_season', data),\n",
    "            add_category('home_away', data),\n",
    "            add_category('team_1', data),\n",
    "            add_category('hour_of_start', data),\n",
    "            add_category('day_of_week', data),\n",
    "            add_category('number_of_game_today', data),\n",
    "            add_category('day_night', data),\n",
    "            add_category('park_id', data),\n",
    "            add_category('this_team_league', data),\n",
    "            add_category('opponent_league', data),\n",
    "        ],\n",
    "        1\n",
    "    )\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ccdeee40-2e0e-4753-ab80-5ba4a82b4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(input_df):\n",
    "    data = input_df\n",
    "    train_data = np.concatenate(\n",
    "        [\n",
    "            add_numeric('barstool_1_odds', data),\n",
    "            add_numeric('betclic_1_odds', data),\n",
    "            add_numeric('betfair_1_odds', data),\n",
    "            add_numeric('betfred_1_odds', data),\n",
    "            add_numeric('betmgm_1_odds', data),\n",
    "            add_numeric('betonlineag_1_odds', data),\n",
    "            add_numeric('betrivers_1_odds', data),\n",
    "            add_numeric('betus_1_odds', data),\n",
    "            add_numeric('betway_1_odds', data),\n",
    "            add_numeric('bovada_1_odds', data),\n",
    "            add_numeric('casumo_1_odds', data),\n",
    "            add_numeric('circasports_1_odds', data),\n",
    "            add_numeric('coral_1_odds', data),\n",
    "            add_numeric('draftkings_1_odds', data),\n",
    "            add_numeric('fanduel_1_odds', data),\n",
    "            add_numeric('foxbet_1_odds', data),\n",
    "            add_numeric('gtbets_1_odds', data),\n",
    "            add_numeric('ladbrokes_1_odds', data),\n",
    "            add_numeric('lowvig_1_odds', data),\n",
    "            add_numeric('marathonbet_1_odds', data),\n",
    "            add_numeric('matchbook_1_odds', data),\n",
    "            add_numeric('mrgreen_1_odds', data),\n",
    "            add_numeric('mybookieag_1_odds', data),\n",
    "            add_numeric('nordicbet_1_odds', data),\n",
    "            add_numeric('onexbet_1_odds', data),\n",
    "            add_numeric('paddypower_1_odds', data),\n",
    "            add_numeric('pinnacle_1_odds', data),\n",
    "            add_numeric('pointsbetus_1_odds', data),\n",
    "            add_numeric('sport888_1_odds', data),\n",
    "            add_numeric('sugarhouse_1_odds', data),\n",
    "            add_numeric('superbook_1_odds', data),\n",
    "            add_numeric('twinspires_1_odds', data),\n",
    "            add_numeric('unibet_1_odds', data),\n",
    "            add_numeric('unibet_eu_1_odds', data),\n",
    "            add_numeric('unibet_uk_1_odds', data),\n",
    "            add_numeric('unibet_us_1_odds', data),\n",
    "            add_numeric('williamhill_1_odds', data),\n",
    "            add_numeric('williamhill_us_1_odds', data),\n",
    "            add_numeric('wynnbet_1_odds', data),\n",
    "            add_numeric('minutes_since_commence', data),\n",
    "            add_numeric('this_team_game_of_season', data),\n",
    "            add_numeric('opponent_game_of_season', data),\n",
    "            add_category('home_away', data),\n",
    "            add_category('team_1', data),\n",
    "            add_category('hour_of_start', data),\n",
    "            add_category('day_of_week', data),\n",
    "            add_category('number_of_game_today', data),\n",
    "            add_category('day_night', data),\n",
    "            add_category('park_id', data),\n",
    "            add_category('this_team_league', data),\n",
    "            add_category('opponent_league', data),\n",
    "        ],\n",
    "        1\n",
    "    )\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "5e16e080-a899-4720-ae0c-ea0d46d6f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data(input_df):\n",
    "    data = input_df\n",
    "    test_data = np.concatenate(\n",
    "        [\n",
    "            add_numeric('barstool_1_odds', data),\n",
    "            add_numeric('betclic_1_odds', data),\n",
    "            add_numeric('betfair_1_odds', data),\n",
    "            add_numeric('betfred_1_odds', data),\n",
    "            add_numeric('betmgm_1_odds', data),\n",
    "            add_numeric('betonlineag_1_odds', data),\n",
    "            add_numeric('betrivers_1_odds', data),\n",
    "            add_numeric('betus_1_odds', data),\n",
    "            add_numeric('betway_1_odds', data),\n",
    "            add_numeric('bovada_1_odds', data),\n",
    "            add_numeric('casumo_1_odds', data),\n",
    "            add_numeric('circasports_1_odds', data),\n",
    "            add_numeric('coral_1_odds', data),\n",
    "            add_numeric('draftkings_1_odds', data),\n",
    "            add_numeric('fanduel_1_odds', data),\n",
    "            add_numeric('foxbet_1_odds', data),\n",
    "            add_numeric('gtbets_1_odds', data),\n",
    "            add_numeric('ladbrokes_1_odds', data),\n",
    "            add_numeric('lowvig_1_odds', data),\n",
    "            add_numeric('marathonbet_1_odds', data),\n",
    "            add_numeric('matchbook_1_odds', data),\n",
    "            add_numeric('mrgreen_1_odds', data),\n",
    "            add_numeric('mybookieag_1_odds', data),\n",
    "            add_numeric('nordicbet_1_odds', data),\n",
    "            add_numeric('onexbet_1_odds', data),\n",
    "            add_numeric('paddypower_1_odds', data),\n",
    "            add_numeric('pinnacle_1_odds', data),\n",
    "            add_numeric('pointsbetus_1_odds', data),\n",
    "            add_numeric('sport888_1_odds', data),\n",
    "            add_numeric('sugarhouse_1_odds', data),\n",
    "            add_numeric('superbook_1_odds', data),\n",
    "            add_numeric('twinspires_1_odds', data),\n",
    "            add_numeric('unibet_1_odds', data),\n",
    "            add_numeric('unibet_eu_1_odds', data),\n",
    "            add_numeric('unibet_uk_1_odds', data),\n",
    "            add_numeric('unibet_us_1_odds', data),\n",
    "            add_numeric('williamhill_1_odds', data),\n",
    "            add_numeric('williamhill_us_1_odds', data),\n",
    "            add_numeric('wynnbet_1_odds', data),\n",
    "            add_numeric('minutes_since_commence', data),\n",
    "            add_numeric('this_team_game_of_season', data),\n",
    "            add_numeric('opponent_game_of_season', data),\n",
    "            \n",
    "            add_category('home_away', data),\n",
    "            add_category('team_1', data),\n",
    "            add_category('hour_of_start', data),\n",
    "            add_category('day_of_week', data),\n",
    "            add_category('number_of_game_today', data),\n",
    "            add_category('day_night', data),\n",
    "            add_category('park_id', data),\n",
    "            add_category('this_team_league', data),\n",
    "            add_category('opponent_league', data),\n",
    "        ],\n",
    "        1\n",
    "    )\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "6b5ca860",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['target']\n",
    "df = df.drop('target', axis='columns')\n",
    "full_data = make_full_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "6c391959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.66, 1.68, 1.72, ..., 1.67, 1.65, 1.67],\n",
       "       [1.66, 1.68, 1.71, ..., 1.67, 1.65, 1.67],\n",
       "       [1.66, 1.68, 1.71, ..., 1.67, 1.65, 1.67],\n",
       "       ...,\n",
       "       [0.  , 0.  , 2.22, ..., 2.15, 2.2 , 0.  ],\n",
       "       [0.  , 0.  , 2.22, ..., 2.15, 2.2 , 0.  ],\n",
       "       [0.  , 0.  , 2.22, ..., 2.15, 2.2 , 0.  ]])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data[:, :39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "e006675c-e96b-479f-8ed2-2eaacb72a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indices of the columns you want to standardize and those we don't\n",
    "continuous_vars = full_data[:, :42]\n",
    "categorical_vars = full_data[:, 42:]\n",
    "\n",
    "#continuous_vars_full = full_data[continuous_vars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "4a7e364d-ba18-49f3-89a0-7032f30fe5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of StandardScaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(continuous_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "475f7eac-083b-4332-b2e0-537753254d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the columns of the training data\n",
    "# X_train_s = np.hstack((scaler.transform(continuous_vars_train), X_train.iloc[:, 42:].values))\n",
    "# X_test_s = np.hstack((scaler.transform(continuous_vars_test), X_test.iloc[:, 42:].values))\n",
    "standardized_data = np.hstack((scaler.transform(continuous_vars), categorical_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "5735168a-0cc9-4ec2-9e3e-0eb8a49e0973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1158646, 137)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "d6ab2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert input data to numpy arrays\n",
    "X_train_np = X_train.astype(np.float32)\n",
    "X_test_np = X_test.astype(np.float32)\n",
    "y_train_np = y_train.values.astype(np.float32)  # Convert y_train to numpy array\n",
    "y_test_np = y_test.values.astype(np.float32)    # Convert y_test to numpy array\n",
    "\n",
    "# Create Torch datasets with the numpy arrays\n",
    "train_data = torch.utils.data.TensorDataset(torch.tensor(X_train_np), torch.tensor(y_train_np))\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_test_np), torch.tensor(y_test_np))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "1894c9bd-ade7-47ef-9a65-45cce1836c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loaders for each of our datasets \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 128, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "6495d47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(926916, 137)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "510a2e49-65ea-4b6a-a1a9-0cdb8a34cf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81168"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the layers and activation functions of our model\n",
    "model = torch.nn.Sequential(   \n",
    "    torch.nn.Linear(137,256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256,128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16,1)\n",
    ")\n",
    "params_count = (137*256)+(256*128)+(128*64)+(64*64)+(64*16)+16\n",
    "params_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "efed2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the layers and activation functions of our model\n",
    "# model = torch.nn.Sequential(   \n",
    "#     torch.nn.Linear(137,256),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(256,128),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(128,16),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(16,1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "151bc190-5357-4fa6-ba2a-842a0d69b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the layers and activation functions of our model\n",
    "# model = torch.nn.Sequential(   \n",
    "#     torch.nn.Linear(137,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,800),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(800,256),\n",
    "#     torch.nn.SiLU(),\n",
    "#     torch.nn.Linear(256,1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "e2551905-6e36-4219-bf87-b7518277eaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2234656"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_count = (137*800)+(800*800)+(800*800)+(800*800)+(800*256)+256\n",
    "params_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "3943e9c8-af08-45db-bc6d-ad7b28e5aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines our scoring function\n",
    "def scoring_function(pred, label):\n",
    "    return nn.functional.binary_cross_entropy_with_logits(pred, label)\n",
    "\n",
    "# Defines number of epochs we want to train through\n",
    "num_epochs = 10\n",
    "\n",
    "# Defines our optimizer and the learning rate \n",
    "optimizer = torch.optim.Adam( model.parameters(), lr=.001  )#, weight_decay=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "4425cb01-94bd-41dd-a102-f3ea5ef78e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_stats(c_tensor, i_tensor):\n",
    "    unscaled_correct = scaler.inverse_transform(c_tensor[:, :42]) if len(c_tensor) > 0 else np.array([]) # unscale the continuous variables\n",
    "    unscaled_incorrect = scaler.inverse_transform(i_tensor[:, :42]) if len(i_tensor) > 0 else np.array([]) # unscale the continuous variables\n",
    "\n",
    "    amt_correct_predictions = len(unscaled_correct) # amount of correct predictions\n",
    "    amt_incorrect_predictions = len(unscaled_incorrect) # amount of incorrect predictions\n",
    "    amt_total_bets = (amt_correct_predictions + amt_incorrect_predictions) # amount of total bets placed\n",
    "\n",
    "    if len(unscaled_correct) > 0:\n",
    "        unscaled_correct = unscaled_correct[:, :39] # only odds data\n",
    "        sum = 0 # calculate the average market odds\n",
    "        count = 0\n",
    "        for row in unscaled_correct:\n",
    "            for val in row:\n",
    "                if val > 0.01: # don't average the 'missing' (0) values \n",
    "                    sum += val\n",
    "                    count +=1\n",
    "        average_odds_won = sum / count\n",
    "    else:\n",
    "        average_odds_won = 0\n",
    "\n",
    "    money_profited = ((amt_correct_predictions * average_odds_won * 100) - (100 * amt_correct_predictions))\n",
    "\n",
    "    money_lost = (amt_incorrect_predictions * 100)\n",
    "\n",
    "    p_l = money_profited - money_lost\n",
    "\n",
    "    correct_pred_percet = amt_correct_predictions/amt_total_bets\n",
    "\n",
    "    ev_per_bet = p_l/amt_total_bets\n",
    "\n",
    "    return p_l, correct_pred_percet, ev_per_bet, amt_total_bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "fe431f8d-9002-4e9d-8962-4d8fe2b424f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initializes a list that will contain our batch losses for an individual epoch\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Defines how we want to step through each batch in the epoch\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # Resets the gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare the input and output tensors for the current batch\n",
    "        batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
    "        batchY = torch.tensor(batch[1], dtype=torch.float32)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model.forward(batchX)\n",
    "        batchY = batchY.unsqueeze(1)  # Reshape to (batch_size, 1)\n",
    "        \n",
    "        # Compute the loss with weighted BCEWithLogitsLoss\n",
    "        pos_weight = torch.tensor([10.0])  # higher weight for positive class\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(y_pred, batchY)\n",
    "        \n",
    "        # Store the loss for this batch in the list\n",
    "        epoch_losses.append(loss.detach().clone())\n",
    "\n",
    "        # Compute the gradient of the error with respect to the model parameters\n",
    "        loss.mean().backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    all_epoch_loss = torch.tensor(epoch_losses)\n",
    "    epoch_loss = torch.mean(all_epoch_loss)\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "04109a9f-401f-485c-be10-b990a3323b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Loop over the test data\n",
    "    for batch in test_loader:\n",
    "        # Prepare the input and output tensors for the current batch\n",
    "        batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
    "        batchY = torch.tensor(batch[1], dtype=torch.float32)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model(batchX)\n",
    "\n",
    "        # Apply threshold\n",
    "        # TODO: HYPERPARAM\n",
    "        predictions = torch.where(predictions > 0.75, 1, 0)\n",
    "\n",
    "        correct_pred_rows = batchX[(predictions.squeeze() == 1) & (batchY.squeeze() == 1)] # find all rows where we make a correct win prediction\n",
    "\n",
    "        incorrect_pred_rows = batchX[(predictions.squeeze() == 1) & (batchY.squeeze() == 0)] # find all rows where we make an incorrect win prediction\n",
    "\n",
    "        p_l, correct_pred_percet, ev_per_bet, amt_total_bets = calc_epoch_stats(correct_pred_rows, incorrect_pred_rows)\n",
    "\n",
    "    # Return the results\n",
    "    return p_l, correct_pred_percet, ev_per_bet, amt_total_bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "f22a687b-095d-4df9-a8a2-7cc270b856e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss: 1.5318089723587036, WP Acc:  59.090909%, WPs: 44, EV: 16.29769000769327, P_L: 717.0983603385039\n",
      "Epoch : 2, Loss: 1.1831661462783813, WP Acc:  62.857143%, WPs: 35, EV: 158.88612857711524, P_L: 5561.014500199033\n",
      "Epoch : 3, Loss: 0.741929292678833, WP Acc:  72.413793%, WPs: 29, EV: 38.396216038391685, P_L: 1113.4902651133589\n",
      "Epoch : 4, Loss: 0.5299798250198364, WP Acc:  84.848485%, WPs: 33, EV: 65.80093968718258, P_L: 2171.431009677025\n",
      "Epoch : 5, Loss: 0.4128592014312744, WP Acc:  83.333333%, WPs: 30, EV: 207.3689766570811, P_L: 6221.069299712433\n",
      "Epoch : 6, Loss: 0.33861300349235535, WP Acc:  80.645161%, WPs: 31, EV: 42.919755500695096, P_L: 1330.512420521548\n",
      "Epoch : 7, Loss: 0.2918685972690582, WP Acc:  90.000000%, WPs: 30, EV: 62.97476042330685, P_L: 1889.2428126992054\n",
      "Epoch : 8, Loss: 0.2564287781715393, WP Acc:  92.592593%, WPs: 27, EV: 98.18694809972385, P_L: 2651.047598692544\n",
      "Epoch : 9, Loss: 0.23346322774887085, WP Acc:  95.833333%, WPs: 24, EV: 88.63225343575762, P_L: 2127.174082458183\n",
      "Epoch : 10, Loss: 0.22183267772197723, WP Acc:  93.939394%, WPs: 33, EV: 70.83497762105728, P_L: 2337.55426149489\n"
     ]
    }
   ],
   "source": [
    "# Calls the train function for each of our epochs, prints the running results\n",
    "for epoch in range(num_epochs):\n",
    "    ep_result = train()\n",
    "    p_l, correct_pred_percet, ev_per_bet, amt_total_bets = test_model()\n",
    "    \n",
    "    print(f'Epoch : {epoch + 1}, Loss: {ep_result}, WP Acc: {correct_pred_percet: 2%}, WPs: {amt_total_bets}, EV: {ev_per_bet}, P_L: {p_l}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff3b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814d90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8fa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a570fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac954fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd039e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72a515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3e75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896ecdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cec84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780ef9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f92485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554dbcee-2ae6-4855-af9f-a65897dcc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = test_data[:, 20:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c68291-b93c-4164-a875-2ed7b4bd9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecfde7-dac8-4c9e-9908-bbb6e235f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts for each category\n",
    "counts = np.sum(teams, axis=0)\n",
    "\n",
    "print(counts)\n",
    "\n",
    "# Plot the distribution using a bar chart\n",
    "labels = [i for i in range(30)]\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Distribution of Categorical Data')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution using a histogram\n",
    "categories = np.argmax(one_hot_data, axis=1)\n",
    "plt.hist(categories, bins=3, align='left', rwidth=0.5)\n",
    "plt.xticks([0, 1, 2], labels)\n",
    "plt.title('Distribution of Categorical Data')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b9ecb-5375-4abe-b84d-003a3d3411d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)  # convert y_test to a numpy array\n",
    "counts_1 = np.sum(teams[y_test.astype(int) == 1], axis=0)\n",
    "counts_0 = np.sum(teams[y_test.astype(int) == 0], axis=0)\n",
    "counts = np.sum(teams, axis=0)\n",
    "\n",
    "print(counts_1)\n",
    "print(counts_0)\n",
    "print(counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e2967-6334-450d-98c3-041d6aaf48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bars in stack manner\n",
    "x = [i for i in range(30)]\n",
    "plt.bar(x, counts_1, color='r')\n",
    "#plt.bar(x, counts_0, bottom=counts_1, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a539f-b320-4590-9f02-1715bd2655cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
