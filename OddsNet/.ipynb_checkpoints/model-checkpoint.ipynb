{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3a5f9b5f-7f90-4c7a-b099-dd4fe78403bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f3e4ad16-e7c3-4b32-ba25-46e3df59e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/stefanfeiler/Desktop/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "eb00abfc-1ea5-4005-ae34-5a9e1cd890d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to seperate 80% of games and 20% of games for test and train splits and then confirm their counts are proportional to those values \n",
    "\n",
    "# Step 1: Get a list of unique groups\n",
    "groups = list(df.groupby('game_id').groups.keys())\n",
    "\n",
    "# Step 2: Shuffle the list of unique groups randomly\n",
    "random.shuffle(groups)\n",
    "\n",
    "# Step 3: Calculate the number of groups for the 20% DataFrame and the 80% DataFrame\n",
    "n_groups_20_percent = int(len(groups) * 0.2)\n",
    "n_groups_80_percent = len(groups) - n_groups_20_percent\n",
    "\n",
    "# Step 4: Use the loc accessor to select rows for each group and add them to the appropriate DataFrame\n",
    "df_20_percent = pd.DataFrame()\n",
    "df_80_percent = pd.DataFrame()\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "    if i < n_groups_20_percent:\n",
    "        df_20_percent = pd.concat([df_20_percent, df.loc[df['game_id'] == group]])\n",
    "    else:\n",
    "        df_80_percent = pd.concat([df_80_percent, df.loc[df['game_id'] == group]])\n",
    "\n",
    "# Optional: Reset the index of the resulting DataFrames\n",
    "df_20_percent = df_20_percent.reset_index(drop=True)\n",
    "df_80_percent = df_80_percent.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4f120a84-c838-4a12-a381-722c2a4bd863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['team_1', 'barstool_team_1_prob', 'betfair_team_1_prob',\n",
       "       'betmgm_team_1_prob', 'betonlineag_team_1_prob',\n",
       "       'betrivers_team_1_prob', 'bovada_team_1_prob',\n",
       "       'circasports_team_1_prob', 'draftkings_team_1_prob',\n",
       "       'fanduel_team_1_prob', 'foxbet_team_1_prob', 'gtbets_team_1_prob',\n",
       "       'pinnacle_team_1_prob', 'pointsbetus_team_1_prob',\n",
       "       'sugarhouse_team_1_prob', 'twinspires_team_1_prob',\n",
       "       'unibet_team_1_prob', 'williamhillus_team_1_prob',\n",
       "       'wynnbet_team_1_prob', 'game_id', 'winning_team',\n",
       "       'minutes_since_commence', 'snapshot_time_taken', 'hour_of_start',\n",
       "       'day_of_week', 'barstool_last_update_time', 'betfair_last_update_time',\n",
       "       'betmgm_last_update_time', 'betonlineag_last_update_time',\n",
       "       'betrivers_last_update_time', 'bovada_last_update_time',\n",
       "       'circasports_last_update_time', 'draftkings_last_update_time',\n",
       "       'fanduel_last_update_time', 'foxbet_last_update_time',\n",
       "       'gtbets_last_update_time', 'pinnacle_last_update_time',\n",
       "       'pointsbetus_last_update_time', 'sugarhouse_last_update_time',\n",
       "       'twinspires_last_update_time', 'unibet_last_update_time',\n",
       "       'williamhillus_last_update_time', 'wynnbet_last_update_time', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4198a5c4-410a-41a1-9990-0aa94147f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_category(column_name, data):\n",
    "    \n",
    "    column_index = data.columns.tolist().index(column_name)\n",
    "    \n",
    "    arr = data[column_name].values.reshape(-1,1)\n",
    "    \n",
    "    coder = OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "    onehots = coder.fit_transform(arr)\n",
    "    \n",
    "    print(onehots.shape)\n",
    "    \n",
    "    return onehots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f47e2b58-e372-4faa-987e-973b5b144717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numeric(column_name, data):\n",
    "    \n",
    "    column_index = data.columns.tolist().index(column_name)\n",
    "    \n",
    "    arr = data[column_name].values.reshape(-1,1).astype('float')\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "586ff161-3075-44c5-a4a8-492d1e7895ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87562, 30)\n",
      "(87562, 9)\n",
      "(87562, 7)\n"
     ]
    }
   ],
   "source": [
    "data = df\n",
    "full_data = np.concatenate(\n",
    "    [\n",
    "        add_numeric('barstool_team_1_prob', data),\n",
    "        add_numeric('betfair_team_1_prob', data),\n",
    "        add_numeric('betmgm_team_1_prob', data),\n",
    "        add_numeric('betonlineag_team_1_prob', data),\n",
    "        add_numeric('betrivers_team_1_prob', data),\n",
    "        add_numeric('bovada_team_1_prob', data),\n",
    "        add_numeric('circasports_team_1_prob', data),\n",
    "        add_numeric('draftkings_team_1_prob', data),\n",
    "        add_numeric('fanduel_team_1_prob', data),\n",
    "        add_numeric('foxbet_team_1_prob', data),\n",
    "        add_numeric('gtbets_team_1_prob', data),\n",
    "        add_numeric('pinnacle_team_1_prob', data),\n",
    "        add_numeric('pointsbetus_team_1_prob', data),\n",
    "        add_numeric('sugarhouse_team_1_prob', data),\n",
    "        add_numeric('twinspires_team_1_prob', data),\n",
    "        add_numeric('unibet_team_1_prob', data),\n",
    "        add_numeric('williamhillus_team_1_prob', data),\n",
    "        add_numeric('wynnbet_team_1_prob', data),\n",
    "        add_numeric('minutes_since_commence', data),\n",
    "        add_category('team_1', data),\n",
    "        add_category('hour_of_start', data),\n",
    "        add_category('day_of_week', data),\n",
    "    ],\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ccdeee40-2e0e-4753-ab80-5ba4a82b4322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64464, 30)\n",
      "(64464, 9)\n",
      "(64464, 7)\n"
     ]
    }
   ],
   "source": [
    "data = df_80_percent\n",
    "train_data = np.concatenate(\n",
    "    [\n",
    "        add_numeric('barstool_team_1_prob', data),\n",
    "        add_numeric('betfair_team_1_prob', data),\n",
    "        add_numeric('betmgm_team_1_prob', data),\n",
    "        add_numeric('betonlineag_team_1_prob', data),\n",
    "        add_numeric('betrivers_team_1_prob', data),\n",
    "        add_numeric('bovada_team_1_prob', data),\n",
    "        add_numeric('circasports_team_1_prob', data),\n",
    "        add_numeric('draftkings_team_1_prob', data),\n",
    "        add_numeric('fanduel_team_1_prob', data),\n",
    "        add_numeric('foxbet_team_1_prob', data),\n",
    "        add_numeric('gtbets_team_1_prob', data),\n",
    "        add_numeric('pinnacle_team_1_prob', data),\n",
    "        add_numeric('pointsbetus_team_1_prob', data),\n",
    "        add_numeric('sugarhouse_team_1_prob', data),\n",
    "        add_numeric('twinspires_team_1_prob', data),\n",
    "        add_numeric('unibet_team_1_prob', data),\n",
    "        add_numeric('williamhillus_team_1_prob', data),\n",
    "        add_numeric('wynnbet_team_1_prob', data),\n",
    "        add_numeric('minutes_since_commence', data),\n",
    "        add_category('team_1', data),\n",
    "        add_category('hour_of_start', data),\n",
    "        add_category('day_of_week', data),\n",
    "    ],\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "5e16e080-a899-4720-ae0c-ea0d46d6f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23098, 30)\n",
      "(23098, 9)\n",
      "(23098, 7)\n"
     ]
    }
   ],
   "source": [
    "data = df_20_percent\n",
    "test_data = np.concatenate(\n",
    "    [\n",
    "        add_numeric('barstool_team_1_prob', data),\n",
    "        add_numeric('betfair_team_1_prob', data),\n",
    "        add_numeric('betmgm_team_1_prob', data),\n",
    "        add_numeric('betonlineag_team_1_prob', data),\n",
    "        add_numeric('betrivers_team_1_prob', data),\n",
    "        add_numeric('bovada_team_1_prob', data),\n",
    "        add_numeric('circasports_team_1_prob', data),\n",
    "        add_numeric('draftkings_team_1_prob', data),\n",
    "        add_numeric('fanduel_team_1_prob', data),\n",
    "        add_numeric('foxbet_team_1_prob', data),\n",
    "        add_numeric('gtbets_team_1_prob', data),\n",
    "        add_numeric('pinnacle_team_1_prob', data),\n",
    "        add_numeric('pointsbetus_team_1_prob', data),\n",
    "        add_numeric('sugarhouse_team_1_prob', data),\n",
    "        add_numeric('twinspires_team_1_prob', data),\n",
    "        add_numeric('unibet_team_1_prob', data),\n",
    "        add_numeric('williamhillus_team_1_prob', data),\n",
    "        add_numeric('wynnbet_team_1_prob', data),\n",
    "        add_numeric('minutes_since_commence', data),\n",
    "        add_category('team_1', data),\n",
    "        add_category('hour_of_start', data),\n",
    "        add_category('day_of_week', data),\n",
    "    ],\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "36ce04d2-f2c9-4bf3-84ad-b351355da5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87562, 65)\n"
     ]
    }
   ],
   "source": [
    "print(full_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e006675c-e96b-479f-8ed2-2eaacb72a2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64464, 46)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the indices of the columns you want to standardize\n",
    "continuous_vars_full = full_data[:, :19]\n",
    "continuous_vars_test = test_data[:, :19]\n",
    "continuous_vars_train = train_data[:, :19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4a7e364d-ba18-49f3-89a0-7032f30fe5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of StandardScaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(continuous_vars_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "475f7eac-083b-4332-b2e0-537753254d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the columns of the training data\n",
    "X_train = np.hstack((scaler.transform(continuous_vars_train), train_data[:, 19:-1]))\n",
    "X_test = np.hstack((scaler.transform(continuous_vars_test), test_data[:, 19:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5735168a-0cc9-4ec2-9e3e-0eb8a49e0973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "254bb6e7-2856-4a37-89ad-c8ce95b2e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our y var\n",
    "y_train = df_80_percent['target'].values\n",
    "y_test = df_20_percent['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c0f3721d-7ea2-4112-a2f1-78f5d07ed5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Torch datasets with our splits\n",
    "train_data = torch.utils.data.TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float())\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "1894c9bd-ade7-47ef-9a65-45cce1836c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loaders for each of our datasets \n",
    "loader = torch.utils.data.DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "510a2e49-65ea-4b6a-a1a9-0cdb8a34cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the layers and activation functions of our model\n",
    "model = torch.nn.Sequential(   \n",
    "    torch.nn.Linear(64,800),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(800,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "3943e9c8-af08-45db-bc6d-ad7b28e5aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines our scoring function\n",
    "def scoring_function(pred, label):\n",
    "    return F.binary_cross_entropy(pred, label)\n",
    "\n",
    "# Defines number of epochs we want to train through\n",
    "num_epochs = 10\n",
    "\n",
    "# Defines our optimizer and the learning rate \n",
    "optimizer = torch.optim.Adam( model.parameters(), .001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "fe431f8d-9002-4e9d-8962-4d8fe2b424f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():  \n",
    "    \n",
    "    # Initializes a list that will contain our batch losses for an individual epoch\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Defines how we want to step through each batch in the epoch\n",
    "    for batch in loader:\n",
    "        \n",
    "        # Resets the grdient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare the input and output tensors for the current batch\n",
    "        batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
    "        batchY = torch.tensor(batch[1], dtype=torch.float32)\n",
    "        #batchY = batchY.unsqueeze(1)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model.forward(batchX)\n",
    "        \n",
    "        #print(y_pred)\n",
    "        #print(batchY[0])\n",
    "        \n",
    "        batchY = batchY.reshape(-1, 1)\n",
    "        \n",
    "        batchY.reshape(-1)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = scoring_function(y_pred, batchY)\n",
    "        \n",
    "        # Store the loss for this batch in the list\n",
    "        epoch_losses.append(loss.detach().clone())\n",
    "\n",
    "        # Compute the gradient of the error with respect to the model parameters\n",
    "        loss.mean().backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    all_epoch_loss = torch.tensor(epoch_losses)\n",
    "    return all_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "f22a687b-095d-4df9-a8a2-7cc270b856e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/45/vvds_7n55h151_y61fjd1fmc0000gn/T/ipykernel_16296/2270616969.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batchX = torch.tensor(batch[0], dtype=torch.float32)\n",
      "/var/folders/45/vvds_7n55h151_y61fjd1fmc0000gn/T/ipykernel_16296/2270616969.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batchY = torch.tensor(batch[1], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Error: 0.5270767211914062\n",
      "Epoch 1, Average Error: 0.3349873423576355\n",
      "Epoch 2, Average Error: 0.20958243310451508\n",
      "Epoch 3, Average Error: 0.16215328872203827\n",
      "Epoch 4, Average Error: 0.13778752088546753\n",
      "Epoch 5, Average Error: 0.12171861529350281\n",
      "Epoch 6, Average Error: 0.1116696372628212\n",
      "Epoch 7, Average Error: 0.1025172770023346\n",
      "Epoch 8, Average Error: 0.09650149196386337\n",
      "Epoch 9, Average Error: 0.08949577808380127\n"
     ]
    }
   ],
   "source": [
    "e = 1\n",
    "epoch_index_list = []\n",
    "accuracy_list = []\n",
    "# Calls the train function for each of our epochs, prints the running results\n",
    "for epoch in range(num_epochs):\n",
    "    ep_result = train()\n",
    "    #accuracy = test_model()\n",
    "    #accuracy_list.append(accuracy)\n",
    "    epoch_index_list.append(e)\n",
    "    e+=1\n",
    "    print('Epoch {}, Average Error: {}'.format(epoch, ep_result.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "09836061-2970-4077-b392-6fafc88f16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function such that each prediction is categorized into either favorable or unfavorable review\n",
    "def transform_prediction_to_target_format(num):\n",
    "    if num > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "922230de-2afb-4197-b2f3-4801382a72eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5005628192917135\n",
      "Average prob: 0.3840659548394302\n",
      "1st quartile: 0.3629305176407024\n",
      "2nd quartile: 0.3830253029831613\n",
      "3rd quartile: 0.40457401653105124\n"
     ]
    }
   ],
   "source": [
    "# Initializes an empty list that will contain the modified predictions (favorable = 1, unfavorable = 0)\n",
    "itemized_predictions = []\n",
    "bs_probs = []\n",
    "\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate over the test set and collect the model's predictions\n",
    "predictions = []\n",
    "for batch in test_loader:\n",
    "    \n",
    "    original_data = scaler.inverse_transform(batch[0][:, :19])\n",
    "    original_data = original_data[:, :18]\n",
    "    \n",
    "    \n",
    "    mask = original_data > 0 #.00\n",
    "    \n",
    "    filtered_arr = original_data[mask]\n",
    "    \n",
    "    # Mask the zeros in each row\n",
    "    #masked_arr = np.ma.masked_equal(original_data[:, :18], 0)\n",
    "    \n",
    "    #print(filtered_arr)\n",
    "\n",
    "    # Calculate the mean of the non-zero values in each row\n",
    "    row_means = np.ma.mean(filtered_arr)\n",
    "    \n",
    "    #print(row_means)\n",
    "    \n",
    "    #first_col = original_data[:, 0]\n",
    "    bs_probs.append(row_means)\n",
    "    \n",
    "    # Get the raw outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch[0])\n",
    "    \n",
    "    # Individually categorizes the predictions\n",
    "    for output in outputs:\n",
    "        #pred = torch.sigmoid(output)\n",
    "        pred = output\n",
    "        itemized_predictions.append(transform_prediction_to_target_format(pred))\n",
    "        \n",
    "# Calculate the accuracy on the test set\n",
    "correct_predictions = 0\n",
    "for i in range(len(itemized_predictions)):\n",
    "    if itemized_predictions[i] == y_test[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "arr = np.array(bs_probs[:-1])\n",
    "print(f\"Average prob: {arr.mean()}\")\n",
    "q1 = np.percentile(arr, 25)\n",
    "q2 = np.percentile(arr, 50)\n",
    "q3 = np.percentile(arr, 75)\n",
    "print(f\"1st quartile: {q1}\")\n",
    "print(f\"2nd quartile: {q2}\")\n",
    "print(f\"3rd quartile: {q3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "4b1aca40-1d88-46a7-beb7-7b68f675786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = (160*.49) - (100 * (1-0.38412074891674536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "7dfc571f-25f6-4321-9640-ad4951b639be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.812074891674534\n"
     ]
    }
   ],
   "source": [
    "print(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbde89-d7b7-428d-9b1c-580d5a1911c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
